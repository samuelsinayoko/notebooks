{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# The Little Book of Semaphores\n",
    "\n",
    "Working through my own implementations of concurrency and synchronization problems from the [Little Book of Semaphores](http://greenteapress.com/semaphores/LittleBookOfSemaphores.pdf) by A. B. Downey.\n",
    "\n",
    "**Asyncio solutions in notebook**\n",
    "\n",
    "We will implement examples using asyncio. \n",
    "\n",
    "**Reference solutions using Sync GUI app**\n",
    "\n",
    "Solutions are also available from https://github.com/AllenDowney/LittleBookOfSemaphores/tree/master/code/sync_code and can be run using the Sync program provided in that repository:\n",
    "```\n",
    "git clone git@github.com:AllenDowney/LittleBookOfSemaphores.git\n",
    "cd LittleBookOfSemaphores/code\n",
    "python Sync.py sync_code/signal.py\n",
    "```\n",
    "![signalling problem using Downey's Sync app](sync_app.png)\n",
    "```\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import logging \n",
    "from IPython.core.debugger import set_trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "async def run_test(test_coroutine, attempts):\n",
    "    \"\"\"Run a test multiple times to make sure we don't get lucky.\"\"\"\n",
    "    [await test_coroutine(attempt) for attempt in range(attempts)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Basic patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Signaling\n",
    "**Signaling problem**\n",
    "2 threads/coroutines having to coordinate to do an action in a particular order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "async def test_signaling(attempt):\n",
    "    \"\"\"Check the signaling approach serialises the threads so that actionA \n",
    "    takes place before actionB. \n",
    "    \n",
    "    Push a value from coroutines A then one from coroutine B into a shared queue, \n",
    "    and make sure they've been pushed in that order. \n",
    "    \"\"\"\n",
    "    # Not using a Lock here because we want to signal/release before waiting/acquiring \n",
    "    # so we need a semaphore. \n",
    "    first_action_done = asyncio.Semaphore(0)\n",
    "    queue = asyncio.Queue()\n",
    "    \n",
    "    async def push(value):\n",
    "        await queue.put(value)\n",
    "\n",
    "    async def coroutineA():\n",
    "        \"\"\"Do first action then signal to B that we're done\"\"\"\n",
    "        asyncio.Task.current_task().name = \"coroutineA\"\n",
    "        await push('A')\n",
    "        first_action_done.release()\n",
    "\n",
    "    async def coroutineB():\n",
    "        asyncio.Task.current_task().name = \"coroutineB\"\n",
    "        async with first_action_done:\n",
    "            res = await push('B')\n",
    "\n",
    "\n",
    "    await asyncio.gather(coroutineA(), coroutineB())\n",
    "    res = [await queue.get() for _ in range(queue.qsize())]\n",
    "    assert res == ['A', 'B'], f'Test failed for attempt {attempt}: got {res}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "await run_test(test_signaling, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Rendez Vous\n",
    "Two threads must await each other before doing some action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "async def test_rendez_vous(attempt):\n",
    "    \"\"\"Make sure action doesn't happen before a rendez vous. \n",
    "    \n",
    "    In this test, the action is reading the key value pairs from a shared dictionary. \n",
    "    Each of the two coroutines adds a key-value pair to the dictionary before the rendez vous. \n",
    "    So if both coroutines have waited for the other one successfully, they should both \n",
    "    return the two same (key,value) pairs. \n",
    "    \"\"\"\n",
    "    b_has_arrived = asyncio.Semaphore(0)\n",
    "    a_has_arrived = asyncio.Semaphore(0)\n",
    "    shared_dict = {}\n",
    "    lock = asyncio.Lock()  # a lock to protect updating the dictionary\n",
    "    \n",
    "    async def read_items_from_dict():\n",
    "        \"\"\"Return tuple of (key, value) pairs giving the items in the dictionary.\"\"\"\n",
    "        async with lock:\n",
    "            res = tuple(sorted(shared_dict.items()))\n",
    "        return res \n",
    "\n",
    "    async def coroutineA(key, value):\n",
    "        \"\"\"Store key value pair in shared dictionary, wait for rendez vous with B, \n",
    "        then read all key value pairs from dictionary. \n",
    "        \"\"\"\n",
    "        a_has_arrived.release()\n",
    "        # put in a value in the queue\n",
    "        async with lock:\n",
    "              shared_dict[key] = value\n",
    "        await b_has_arrived.acquire()\n",
    "        res = await read_items_from_dict()\n",
    "        return res\n",
    "\n",
    "    async def coroutineB(key, value):\n",
    "        \"\"\"Store key value pair in shared dictionary, wait for rendez vous with A, \n",
    "        then read all key value pairs from dictionary. \n",
    "        \"\"\"\n",
    "        b_has_arrived.release()\n",
    "        async with lock:\n",
    "              shared_dict[key] = value\n",
    "        await a_has_arrived.acquire()\n",
    "        res = await read_items_from_dict()\n",
    "        return res\n",
    "\n",
    "    itemsA, itemsB = await asyncio.gather(\n",
    "        coroutineA('A', 1), \n",
    "        coroutineB('B', 2)\n",
    "    )\n",
    "    expected_items = ('A', 1), ('B', 2)\n",
    "    results = {k:v for k,v in locals().items() if k in ['attempt', 'itemsA', 'itemsB', 'expected_items']}\n",
    "    assert itemsA == itemsB == expected_items, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "await run_test(test_rendez_vous, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Mutex\n",
    "Protects access to critical regions. Mutal exclusion: two threads can't both have access to the mutex at the same time. \n",
    "\n",
    "How to test:\n",
    "- Use a lot of threads and make them race hard: see `test_mutex_threadpoool`\n",
    "- Better demonstrated without asyncio as the event loop tends to make some of these issues go away because operations between awaits are atomic (there's only one thread). \n",
    "  - we can demonstrate with asyncio by forcing a context switch: see `test_mutex_aio`\n",
    "  - we also can also run the threadpool test using aio `test_mutex_threadpool_aio`\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Thread pool test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import threading\n",
    "\n",
    "def test_mutex_threadpool():\n",
    "    \"\"\"Check we can protect a variable using a mutex. \n",
    "    \n",
    "    Better demonstrated with threads rather than coroutines because with asyncio's event loop\n",
    "    operations between \"awaits\" are atomic.\n",
    "    \n",
    "    Use a big threadpool and make the threads race hard \n",
    "    by all incrementing the counter many times.\n",
    "     \n",
    "    \"\"\"\n",
    "    max_workers=100\n",
    "    pool = ThreadPoolExecutor(max_workers=max_workers)\n",
    "    counter = 0\n",
    "    lock = threading.Lock()\n",
    "    \n",
    "    iterations = 100_000\n",
    "    def increment_shared_counter(_):\n",
    "        nonlocal counter\n",
    "        with lock:\n",
    "            for _ in range(iterations):\n",
    "                counter += 1\n",
    "        #time.sleep(random.random())\n",
    "        return counter\n",
    "    intermediate_results = list(pool.map(increment_shared_counter, range(max_workers)))\n",
    "    #print(sorted(intermediate_results))\n",
    "    assert counter == iterations * max_workers, {\n",
    "        k:v for k,v in locals().items() if k in \n",
    "        ['counter', 'intermediate_results']\n",
    "    }\n",
    "    \n",
    "test_mutex_threadpool() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Thread pool + asyncio test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "async def test_mutex_threadpool_aio():\n",
    "    \"\"\"Check we can protect a variable using a mutex. \n",
    "    \n",
    "    Use a big threadpool and make the threads race hard \n",
    "    by all incrementing the counter many times. \n",
    "    \n",
    "    Use asyncio to run the experiment via loop.run_in_executor \n",
    "    \"\"\"\n",
    "    pool = ThreadPoolExecutor(max_workers=100)\n",
    "    lock = threading.Lock()\n",
    "    counter = 0\n",
    "\n",
    "    def increment_shared_counter(_):\n",
    "        nonlocal counter\n",
    "        with lock:\n",
    "            for _ in range(100_000):\n",
    "                counter = counter + 1\n",
    "        #time.sleep(random.random())\n",
    "        return counter\n",
    "    \n",
    "    loop = asyncio.get_event_loop()\n",
    "    tasks = [\n",
    "        loop.run_in_executor(pool, increment_shared_counter, i) \n",
    "        for i in range(100)\n",
    "    ]\n",
    "    intermediate_results = await asyncio.gather(*tasks)\n",
    "    #print(sorted(intermediate_results))\n",
    "    assert counter == 100_000 * 100, {\n",
    "        k:v for k,v in locals().items() if k in \n",
    "        ['counter', 'intermediate_results']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "await test_mutex_threadpool_aio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Minimal asyncio test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "async def test_mutex_aio():\n",
    "    \"\"\"Check we can protect a variable using a mutex. \n",
    "    \n",
    "    Minimal test with asyncio to force a context switch between \n",
    "    reading and updating the counter. \n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    lock = asyncio.Lock()\n",
    "    \n",
    "    async def increment_shared_counter():\n",
    "        nonlocal counter\n",
    "        async with lock:   # fails \n",
    "            # this is a bit convoluted but is a simple way to \n",
    "            # simulating non-thread safe concurrent updates to counter:\n",
    "            # we're sleeping between reading and incrementing the value of counter\n",
    "            # which allows a context switch: without the lock this will fail the test\n",
    "            x = counter\n",
    "            # forcing a context switch by sleeping \n",
    "            await asyncio.sleep(0.001)\n",
    "            counter = x + 1\n",
    "        return counter\n",
    "    tasks = [increment_shared_counter() for _ in range(2)]\n",
    "    intermediate_results = await asyncio.gather(*tasks)\n",
    "    #print(sorted(intermediate_results))\n",
    "    assert counter == 1 * 2, {\n",
    "        k:v for k,v in locals().items() if k in \n",
    "        ['counter', 'intermediate_results']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "await test_mutex_aio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Multiplex\n",
    "A kind of bounded mutex where up to a certain number of threads are allowed to own a lock. \n",
    "\n",
    "Think of a bouncer who allows up to certain number of people to get into a nightclub (or a supermarket during Covid 19 outbreak), and bounces people off when the max capacity has been reached. Then whenever someone leaves the room, another person is allowed to get in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "async def test_multiplex(attempt):\n",
    "    \"\"\"Check the max number of active threads in the critical region is as expected.\n",
    "    \n",
    "    Use a counter and a variable to keep track of the max number of threads, and \n",
    "    verify it's <= the size of the multiplex. \n",
    "    \"\"\"\n",
    "    multiplex = asyncio.Semaphore(5)\n",
    "    # we're using asyncio so no real need for locks \n",
    "    # to protect these variables: operations on them will be atomic \n",
    "    # in between awaits. \n",
    "    counter = 0\n",
    "    max_counter = 0\n",
    "    \n",
    "    async def multiplexed(name):\n",
    "        nonlocal counter\n",
    "        nonlocal max_counter\n",
    "        async with multiplex:\n",
    "            counter += 1    # enterning critical zone \n",
    "            if counter >= max_counter:\n",
    "                max_counter = counter\n",
    "            await asyncio.sleep(0.1)  # block to allow context switch\n",
    "            counter -= 1  # exiting critical zone \n",
    "        return max_counter\n",
    "    tasks = [multiplexed(name) for name in range(7)]\n",
    "    intermediate_results = await asyncio.gather(*tasks)\n",
    "    assert max_counter == 5, {\n",
    "        k:v for k,v in locals().items() if k in \n",
    "        ['max_counter', 'intermediate_results', 'attempt']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "await run_test(test_multiplex, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "loop = asyncio.get_event_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Barrier\n",
    "Generalised rendez vous: all coroutines have to wait at the barrier until all of them have arrived. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "async def test_barrier(attempt):\n",
    "    \"\"\"All coroutines must wait at the barrier until they're all waiting behind it. \n",
    "    Then the barrier opens. \n",
    "    \n",
    "    Testing strategy: all coroutines add their own key,value pair to a shared dictionary. \n",
    "    Once a coroutine passes the barrier, it retrieves all key value pairs from the dictionary. \n",
    "    If the barrier's been effective, all coroutines should read the same content made of one key value pair per coroutine. \n",
    "    \"\"\"\n",
    "    barrier = asyncio.Semaphore(0)  \n",
    "    barrier_size = 5         # open barrier if nb threads behind barrier reaches this count\n",
    "    lock = asyncio.Lock()    # to protect access to dictionary\n",
    "    shared_dict = {}\n",
    "    arrived_count = 0\n",
    "    \n",
    "    async def read_items_from_dict():\n",
    "        \"\"\"Return tuple of (key, value) pairs giving the items in the dictionary.\"\"\"\n",
    "        async with lock:\n",
    "            res = tuple(sorted(shared_dict.items()))\n",
    "        return res\n",
    "    \n",
    "    async def push_wait_then_read(key, value):\n",
    "        nonlocal arrived_count\n",
    "        async with lock:\n",
    "            shared_dict[key] = value \n",
    "        arrived_count += 1\n",
    "        if arrived_count == barrier_size:\n",
    "            barrier.release()\n",
    "        else:\n",
    "            await barrier.acquire()\n",
    "            barrier.release()\n",
    "        items = await read_items_from_dict()\n",
    "        return items \n",
    "    \n",
    "    expected_items = tuple((key, value) for key, value in zip(list('ABCDE'), range(5)))\n",
    "    tasks = [push_wait_then_read(key, value) for (key, value) in expected_items]\n",
    "    received_items_per_coroutine = await asyncio.gather(*tasks)\n",
    "    assert all(\n",
    "        (received_items == expected_items) \n",
    "        for received_items \n",
    "        in received_items_per_coroutine\n",
    "    ), {\n",
    "        k:v for k,v in locals().items() if k in \n",
    "        ['received_items_per_coroutine', 'attempt']\n",
    "    }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "await run_test(test_barrier, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Reusable Barrier\n",
    "Generalised rendez vous: all coroutines have to wait at the barrier until all of them have arrived. Barrier is in a loop so needs to be reused. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "async def test_reusable_barrier(attempt):\n",
    "    \"\"\"All coroutines must wait at the barrier until they're all waiting behind it. \n",
    "    Then the barrier opens. \n",
    "    \n",
    "    Testing strategy: all coroutines add their own key,value pair to a shared dictionary. \n",
    "    Once a coroutine passes the barrier, it retrieves all key value pairs from the dictionary. \n",
    "    If the barrier's been effective, all coroutines should read the same content made of one key value pair per coroutine. \n",
    "    \"\"\"\n",
    "    turnstile = asyncio.Semaphore(0)  \n",
    "    barrier_size = 5         # open barrier if nb threads behind barrier reaches this count\n",
    "    lock = asyncio.Lock()    # to protect access to dictionary\n",
    "    shared_dict = {}\n",
    "    arrived_count = 0\n",
    "    released_count = 0\n",
    "    \n",
    "    async def read_items_from_dict():\n",
    "        \"\"\"Return tuple of (key, value) pairs giving the items in the dictionary.\"\"\"\n",
    "        async with lock:\n",
    "            res = tuple(sorted(shared_dict.items()))\n",
    "        return res\n",
    "    \n",
    "    async def push_wait_then_read(key, value):\n",
    "        nonlocal arrived_count\n",
    "        nonlocal released_count \n",
    "        async with lock:\n",
    "            shared_dict[key] = value \n",
    "        arrived_count += 1\n",
    "        if arrived_count == barrier_size:\n",
    "            turnstile.release()\n",
    "            released_count += 1\n",
    "        else:\n",
    "            await turnstile.acquire()\n",
    "            released_count += 1 \n",
    "            if released_count < barrier_size:\n",
    "                turnstile.release()\n",
    "            else:\n",
    "                # we're done: last thread exiting the turnstile,\n",
    "                # so let's reset the counters so we can reuse \n",
    "                # the barrier again \n",
    "                arrived_count = 0\n",
    "                released_count = 0\n",
    "        items = await read_items_from_dict()\n",
    "        return items \n",
    "    \n",
    "    expected_items = tuple((key, value) for key, value in zip(list('ABCDE'), range(5)))\n",
    "    # first time \n",
    "    tasks1 = [push_wait_then_read(key, value) for (key, value) in expected_items]\n",
    "    received_items_per_coroutine1 = await asyncio.gather(*tasks1)\n",
    "    assert all(\n",
    "        (received_items == expected_items) \n",
    "        for received_items \n",
    "        in received_items_per_coroutine1\n",
    "    ), {\n",
    "        k:v for k,v in locals().items() if k in \n",
    "        ['received_items_per_coroutine1', 'attempt']\n",
    "    }\n",
    "    # second time \n",
    "    shared_dict = {}  # clear the dict\n",
    "    tasks2 = [push_wait_then_read(key, value) for (key, value) in expected_items]\n",
    "    received_items_per_coroutine2 = await asyncio.gather(*tasks2)\n",
    "    \n",
    "    assert all(\n",
    "        (received_items == expected_items) \n",
    "        for received_items \n",
    "        in received_items_per_coroutine2\n",
    "    ), {\n",
    "        k:v for k,v in locals().items() if k in \n",
    "        ['received_items_per_coroutine2', 'attempt']\n",
    "    }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "await run_test(test_reusable_barrier, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Queues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Semaphores can be used to implement queues: use a mutex to protect access to a list, then \n",
    "append to it. To consume the queue, pop it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_queue(attempt):\n",
    "    \"\"\"Rolling our own implementation of a queue using a list and a mutex. \n",
    "    \n",
    "    Test: even though the coroutines will be launched together, we'll \n",
    "    use a counter to ensure the values are enqueued in a specific order. \n",
    "    Then we'll read from the queue and check we're getting the items \n",
    "    back in the expected order. \n",
    "    \n",
    "    \"\"\"\n",
    "    lock = asyncio.Lock()\n",
    "    counter = 0\n",
    "    queue = []  # FIFO\n",
    "    \n",
    "\n",
    "    async def append_items_to_queue(index, value):\n",
    "        \"\"\"Store key value pair in shared dictionary, wait for rendez vous with B, \n",
    "        then read all key value pairs from dictionary. \n",
    "        \"\"\"\n",
    "        nonlocal counter\n",
    "        while True:\n",
    "            if counter == index:\n",
    "                async with lock:\n",
    "                    logger.debug('coroutine %d appending %d to queue.', index, value)\n",
    "                    queue.append(value)\n",
    "                counter += 1\n",
    "                break\n",
    "            else:\n",
    "                logger.debug('coroutine %d waiting for turn.', index)\n",
    "                await asyncio.sleep(0.001)\n",
    "\n",
    "    expected = tuple(i*i for i in range(10))\n",
    "    await asyncio.gather(\n",
    "        *[append_items_to_queue(index, value) for index, value in enumerate(expected)]\n",
    "    )\n",
    "    logging.debug('Reading items from queue, %s.', queue)\n",
    "    result = tuple(queue)\n",
    "    assert result == expected, {\n",
    "        k:v for k,v in locals().items() if k in \n",
    "        ['result', 'expected', 'attempt']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logger.setLevel(logging.DEBUG)\n",
    "#await test_queue(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "await run_test(test_queue, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producers and Consumers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Producers push parameters to queue ready for consumption\n",
    "- Consumers read paramters from queue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readers and Writers\n",
    "- if readers are in critical region, writers can't write \n",
    "- only one writer at a time in critical region\n",
    "- multiple readers at a time allowed in critical retion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mutex: protects access to critical region.\n",
    "- counter: keeps track of number of readers in critical region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_readers_and_writers(attempt):\n",
    "    \"\"\"\n",
    "    \n",
    "    Test 1:\n",
    "    - launch a writer\n",
    "    - launch a bunch of readers\n",
    "    - verify that readers wait until the writer has left the critical zone.\n",
    "    \n",
    "    Test 2:\n",
    "    - launch a bunch of readers\n",
    "    - launch a writer\n",
    "    - verify that the writer waits until the readers have left the critical zone.\n",
    "    \n",
    "    Test 3:\n",
    "    - verify that you don't get two writers in critical zone at the same time. \n",
    "    \"\"\"\n",
    "    \n",
    "    critical_region_empty = asyncio.Lock()\n",
    "    counter_mutex = asyncio.Lock()\n",
    "    nb_readers_in_critical_region = 0\n",
    "    critical = []\n",
    "    \n",
    "    async def do_writing(x, writer_id):\n",
    "        await asyncio.sleep(0.5)\n",
    "        logger.debug(f'writer {writer_id} appending {x} to {critical}')\n",
    "        critical.append(x)\n",
    "        return critical\n",
    "        \n",
    "    async def do_reading(reader_id):\n",
    "        await asyncio.sleep(random.random() * 0.1)\n",
    "        logger.debug(f'reader {reader_id} reading from {critical}')\n",
    "        return list(critical)\n",
    "        \n",
    "    async def writer(writer_id, value, sleep=0):\n",
    "        await asyncio.sleep(sleep)\n",
    "        async with critical_region_empty:\n",
    "            logger.debug(f'writer {writer_id} entering critical region.')\n",
    "            result = await do_writing(value, writer_id)\n",
    "            logger.debug(f'writer {writer_id} leaving critical region.')\n",
    "            return result\n",
    "    \n",
    "    async def reader(reader_id, sleep=0):\n",
    "        await asyncio.sleep(sleep)\n",
    "        nonlocal nb_readers_in_critical_region\n",
    "        logger.debug('reader %s waiting for counter mutex', reader_id)\n",
    "        async with counter_mutex:\n",
    "            if nb_readers_in_critical_region == 0:\n",
    "                # first reader blocks if there's a writer in critical region\n",
    "                await critical_region_empty.acquire()\n",
    "                logger.debug('reader %s locking critical region', reader_id)\n",
    "\n",
    "        logger.debug(f'reader {reader_id} entering critical region.')\n",
    "        nb_readers_in_critical_region += 1\n",
    "        logger.debug(f'{nb_readers_in_critical_region} reader(s) in critical region.')\n",
    "\n",
    "        result = await do_reading(reader_id)\n",
    "        # leave critical region\n",
    "        nb_readers_in_critical_region -= 1\n",
    "        logger.debug('reader %s leaving critical region', reader_id)\n",
    "        if nb_readers_in_critical_region == 0:\n",
    "            # last reader out of critical region releases the lock\n",
    "            logger.debug('reader %s releasing critical region lock', reader_id)\n",
    "            critical_region_empty.release()\n",
    "        return result \n",
    "        \n",
    "    async def test_writer_first():\n",
    "        nonlocal critical\n",
    "        critical = []\n",
    "        short_wait = 1e-5\n",
    "        computed = await asyncio.gather(*[\n",
    "            writer(0, 18),\n",
    "            reader(0, short_wait), \n",
    "            reader(1, short_wait), \n",
    "            reader(2, short_wait)\n",
    "        ])\n",
    "        # if readers waited they should all have gotten 18. \n",
    "        expected = [[18]] * 4  \n",
    "\n",
    "        assert computed == expected, {\n",
    "            k:v for k,v in locals().items() if k in \n",
    "            ['computed', 'expected', 'attempt']\n",
    "        }\n",
    "        assert computed == expected \n",
    "\n",
    "    async def test_readers_first():\n",
    "        nonlocal critical\n",
    "        critical = [42]\n",
    "        short_wait = 1e-5\n",
    "        computed = await asyncio.gather(*[\n",
    "            reader(0), \n",
    "            reader(1), \n",
    "            reader(2),\n",
    "            writer(0, 18, short_wait),\n",
    "\n",
    "        ])\n",
    "        # if readers waited they should all have gotten 18. \n",
    "        expected = [[42]] * 3 + [[42, 18]]  \n",
    "\n",
    "        assert computed == expected, {\n",
    "            k:v for k,v in locals().items() if k in \n",
    "            ['computed', 'expected', 'attempt']\n",
    "        }\n",
    "        assert computed == expected \n",
    "       \n",
    "    await test_readers_first()\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:reader 1 waiting for counter mutex\n",
      "DEBUG:root:reader 1 locking critical region\n",
      "DEBUG:root:reader 1 entering critical region.\n",
      "DEBUG:root:1 reader(s) in critical region.\n",
      "DEBUG:root:reader 2 waiting for counter mutex\n",
      "DEBUG:root:reader 2 entering critical region.\n",
      "DEBUG:root:2 reader(s) in critical region.\n",
      "DEBUG:root:reader 0 waiting for counter mutex\n",
      "DEBUG:root:reader 0 entering critical region.\n",
      "DEBUG:root:3 reader(s) in critical region.\n",
      "DEBUG:root:reader 2 reading from [42]\n",
      "DEBUG:root:reader 2 leaving critical region\n",
      "DEBUG:root:reader 0 reading from [42]\n",
      "DEBUG:root:reader 0 leaving critical region\n",
      "DEBUG:root:reader 1 reading from [42]\n",
      "DEBUG:root:reader 1 leaving critical region\n",
      "DEBUG:root:reader 1 releasing critical region lock\n",
      "DEBUG:root:writer 0 entering critical region.\n",
      "DEBUG:root:writer 0 appending 18 to [42]\n",
      "DEBUG:root:writer 0 leaving critical region.\n"
     ]
    }
   ],
   "source": [
    "logging.getLogger().setLevel(logging.DEBUG)\n",
    "await test_readers_and_writers(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dining Philosophers\n",
    "- 5 philosophers, all executing the following loop\n",
    "```python\n",
    "while  True:\n",
    "    think ()\n",
    "    get_forks()\n",
    "    eat()\n",
    "    put_forks ()\n",
    "```\n",
    "- philosophers need 2 forks to eat\n",
    "- loop stops once all philosophers have eaten\n",
    "- only one philosopher can own a particular fork at any given time\n",
    "- philosphers can't starve\n",
    "- no dealock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def left(i):\n",
    "    return i\n",
    "\n",
    "def right(i):\n",
    "    return (i + 1) % 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sinayoks/apps/miniconda3/envs/vaex/lib/python3.6/site-packages/IPython/core/compilerop.py:101: RuntimeWarning: coroutine 'test_dining_philosophers' was never awaited\n",
      "  return compile(source, filename, symbol, self.flags | PyCF_ONLY_AST, 1)\n"
     ]
    }
   ],
   "source": [
    "async def test_dining_philosophers(attempt):\n",
    "    \n",
    "    forks = [asyncio.Lock() for _ in range(5)]\n",
    "    _fork_location=[None] * 5   # None means table \n",
    "    _has_eaten = [False] * 5\n",
    "    mutex = asyncio.Lock()\n",
    "    \n",
    "    async def philosopher(i):\n",
    "        while True:\n",
    "            #logging.debug('has eaten = %s', _has_eaten)\n",
    "            await think(i)\n",
    "            await get_forks(i)\n",
    "            await eat(i)\n",
    "            await put_forks(i)\n",
    "            if all(_has_eaten):\n",
    "                break\n",
    "            \n",
    "    async def think(i):\n",
    "        logging.debug('philosopher %s is thinking', i)\n",
    "        await asyncio.sleep(random.random() * 0.1)\n",
    "        \n",
    "    \n",
    "    async def eat(i):\n",
    "        if not _has_eaten[i]:\n",
    "            logging.debug('philosopher %s is eating', i)\n",
    "            await asyncio.sleep(random.random() * 0.1)\n",
    "            _has_eaten[i] = True\n",
    "            logging.debug('has eaten = %s', _has_eaten)\n",
    "        \n",
    "    async def get_forks(i):\n",
    "        # if has left, wait right\n",
    "        # if has right, await left\n",
    "        if _has_eaten[i]:\n",
    "            return \n",
    "        logging.debug('philosopher %s waiting for forks', i)\n",
    "        if random.random() < 0.5:\n",
    "            await forks[left(i)].acquire()\n",
    "            async with mutex:\n",
    "                _fork_location[left(i)] = i\n",
    "            await forks[right(i)].acquire()\n",
    "            async with mutex:\n",
    "                _fork_location[right(i)] = i\n",
    "        else:\n",
    "            await forks[right(i)].acquire()\n",
    "            async with mutex:\n",
    "                _fork_location[right(i)] = i\n",
    "            await forks[left(i)].acquire()\n",
    "            async with mutex:\n",
    "                _fork_location[left(i)] = i\n",
    "        logger.debug('fork location = %s', _fork_location)\n",
    "    \n",
    "    async def put_forks(i):\n",
    "        if _has_eaten[i]:\n",
    "            async with mutex:\n",
    "                if _fork_location[left(i)] == i:\n",
    "                    _fork_location[left(i)] = None\n",
    "                    forks[left(i)].release()\n",
    "            async with mutex:\n",
    "                if _fork_location[right(i)] == i:\n",
    "                    _fork_location[right(i)] = None\n",
    "                    forks[right(i)].release()\n",
    "                \n",
    "    \n",
    "    \n",
    "    tasks = [philosopher(i) for i in range(5)]\n",
    "    await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb on\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "await run_test(test_dining_philosophers, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
